{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Cookbook - GPT4o-mini with RAG\n",
    "\n",
    "Instructions before running this notebook:\n",
    "1. pip install -r requirements.txt before running this notebook\n",
    "2. Create a .env file with your OpenAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import ast\n",
    "import tiktoken\n",
    "import concurrent\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "from IPython.display import Image, display, HTML\n",
    "from typing import List\n",
    "\n",
    "# To load OpenAI API Key from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() # Load environment variables from .env file\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "GPT_MODEL = \"gpt-4o-mini\"\n",
    "EMBEDDING_MODEL = \"text-embedding-3-large\"\n",
    "EMBEDDING_COST_PER_1K_TOKENS = 0.00013"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpful Utility functions \n",
    "To create embeddings via batch + parallelized execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Batch Embedding Logic\n",
    "## Parallelize the execution of these embeddings to ensure that the script scales up for larger datasets. \n",
    "## Batchcify() splits input corpus into smaller chunks, embed_corpus() processes these chunks in parallel,\n",
    "## and get_embeddings() makes API calls to OpenAI embedding model.\n",
    "\n",
    "# Function to take in a list of text objects and return them as a list of embeddings\n",
    "# @retry is a decorator from the tenacity library that adds retry logic to the function. \n",
    "# If the function fails (e.g., due to API rate limits or network issues), it will:\n",
    "#       - Wait for a random exponential time between 1 and 40 seconds before retrying\n",
    "#       - Stop after 10 failed attempts\n",
    "#       - This makes  API calls more robust against temporary failures\n",
    "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(10))\n",
    "def get_embeddings(input: List):\n",
    "    response = client.embeddings.create(\n",
    "        input=input,\n",
    "        model=EMBEDDING_MODEL\n",
    "    ).data\n",
    "    return [data.embedding for data in response]\n",
    "\n",
    "# Splits an iterable into batches of size n. \n",
    "# Used in embed_corpus() to split text corpus into manageable chunks\n",
    "def batchify(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx : min(ndx + n, l)]\n",
    "     \n",
    "\n",
    "# Function for batching and parallel processing the embeddings\n",
    "def embed_corpus( \n",
    "    corpus: List[str], \n",
    "    batch_size=64, \n",
    "    num_workers=8, \n",
    "    max_context_len=8191,\n",
    "    ):\n",
    "    # Encode the corpus, truncating to max_context_len\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    encoded_corpus = [\n",
    "        encoded_article[:max_context_len] for encoded_article in encoding.encode_batch(corpus)\n",
    "    ]\n",
    "\n",
    "    # Calculate corpus statistics: the number of inputs, the total number of tokens, and the estimated cost to embed\n",
    "    num_tokens = sum(len(article) for article in encoded_corpus)\n",
    "    cost_to_embed_tokens = num_tokens / 1000 * EMBEDDING_COST_PER_1K_TOKENS\n",
    "    print(\n",
    "        f\"num_articles={len(encoded_corpus)}, num_tokens={num_tokens}, est_embedding_cost={cost_to_embed_tokens:.2f} USD\"\n",
    "    )\n",
    "\n",
    "    # Embed the corpus\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        \n",
    "        futures = [\n",
    "            executor.submit(get_embeddings, text_batch)\n",
    "            for text_batch in batchify(encoded_corpus, batch_size)\n",
    "        ]\n",
    "\n",
    "        with tqdm(total=len(encoded_corpus)) as pbar:\n",
    "            for _ in concurrent.futures.as_completed(futures):\n",
    "                pbar.update(batch_size)\n",
    "\n",
    "        embeddings = []\n",
    "        for future in futures:\n",
    "            data = future.result()\n",
    "            embeddings.extend(data)\n",
    "\n",
    "        return embeddings\n",
    "    \n",
    "\n",
    "# Function to generate embeddings for a given column in a DataFrame\n",
    "def generate_embeddings(df, column_name):\n",
    "    # Initialize an empty list to store embeddings\n",
    "    descriptions = df[column_name].astype(str).tolist()\n",
    "    embeddings = embed_corpus(descriptions)\n",
    "\n",
    "    # Add the embeddings as a new column to the DataFrame\n",
    "    df['embeddings'] = embeddings\n",
    "    print(\"Embeddings created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the unstructured data (csv) into a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id gender masterCategory subCategory articleType baseColour  season  \\\n",
      "0  27152    Men        Apparel     Topwear      Shirts       Blue  Summer   \n",
      "1  10469    Men        Apparel     Topwear     Tshirts     Yellow    Fall   \n",
      "2  17169    Men        Apparel     Topwear      Shirts     Maroon    Fall   \n",
      "3  56702    Men        Apparel     Topwear      Kurtas       Blue  Summer   \n",
      "4  47062  Women        Apparel  Bottomwear     Patiala      Multi    Fall   \n",
      "\n",
      "     year   usage                       productDisplayName  \n",
      "0  2012.0  Formal       Mark Taylor Men Striped Blue Shirt  \n",
      "1  2011.0  Casual   Flying Machine Men Yellow Polo Tshirts  \n",
      "2  2011.0  Casual  U.S. Polo Assn. Men Checks Maroon Shirt  \n",
      "3  2012.0  Ethnic                  Fabindia Men Blue Kurta  \n",
      "4  2012.0  Ethnic        Shree Women Multi Colored Patiala  \n",
      "Opened dataset successfully. Dataset has 1000 items of clothing.\n"
     ]
    }
   ],
   "source": [
    "styles_filepath = \"data/sample_clothes/sample_styles.csv\"\n",
    "styles_df = pd.read_csv(styles_filepath, on_bad_lines='skip')\n",
    "print(styles_df.head())\n",
    "print(\"Opened dataset successfully. Dataset has {} items of clothing.\".format(len(styles_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Embeddings from Dataframe (containing unstructured text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_articles=1000, num_tokens=8280, est_embedding_cost=0.00 USD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1024it [00:05, 175.20it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings created successfully.\n",
      "Writing embeddings to file ...\n",
      "Embeddings successfully stored in sample_styles_with_embeddings.csv\n"
     ]
    }
   ],
   "source": [
    "# Creates embeddings for each product description (productDisplayName) \n",
    "# using the OpenAI Embeddings API via the parallelized technique above\n",
    "generate_embeddings(styles_df, 'productDisplayName')\n",
    "print(\"Writing embeddings to file ...\")\n",
    "styles_df.to_csv('data/sample_clothes/sample_styles_with_embeddings.csv', index=False)\n",
    "print(\"Embeddings successfully stored in sample_styles_with_embeddings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id gender masterCategory subCategory articleType baseColour  season  \\\n",
      "0  27152    Men        Apparel     Topwear      Shirts       Blue  Summer   \n",
      "1  10469    Men        Apparel     Topwear     Tshirts     Yellow    Fall   \n",
      "2  17169    Men        Apparel     Topwear      Shirts     Maroon    Fall   \n",
      "3  56702    Men        Apparel     Topwear      Kurtas       Blue  Summer   \n",
      "4  47062  Women        Apparel  Bottomwear     Patiala      Multi    Fall   \n",
      "\n",
      "     year   usage                       productDisplayName  \\\n",
      "0  2012.0  Formal       Mark Taylor Men Striped Blue Shirt   \n",
      "1  2011.0  Casual   Flying Machine Men Yellow Polo Tshirts   \n",
      "2  2011.0  Casual  U.S. Polo Assn. Men Checks Maroon Shirt   \n",
      "3  2012.0  Ethnic                  Fabindia Men Blue Kurta   \n",
      "4  2012.0  Ethnic        Shree Women Multi Colored Patiala   \n",
      "\n",
      "                                          embeddings  \n",
      "0  [0.006894612684845924, 0.00028893034323118627,...  \n",
      "1  [-0.04374878853559494, -0.008918779902160168, ...  \n",
      "2  [-0.028013937175273895, 0.058833517134189606, ...  \n",
      "3  [-0.0037015778943896294, 0.02956002578139305, ...  \n",
      "4  [-0.05243204906582832, 0.015965517610311508, -...  \n",
      "Opened dataset successfully. Dataset has 1000 items of clothing along with their embeddings.\n"
     ]
    }
   ],
   "source": [
    "## Read the embeddings from the CSV file, load into a DF\n",
    "# styles_df = pd.read_csv('data/sample_clothes/sample_styles_with_embeddings.csv', on_bad_lines='skip')\n",
    "\n",
    "## Convert the 'embeddings' column from string representations of lists to actual lists of floats\n",
    "# styles_df['embeddings'] = styles_df['embeddings'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "print(styles_df.head())\n",
    "print(\"Opened dataset successfully. Dataset has {} items of clothing along with their embeddings.\".format(len(styles_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Matching Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function for cosine similarity\n",
    "def cosine_similarity_manual(vec1, vec2):\n",
    "    \"\"\"Calculate the cosine similarity between two vectors.\"\"\"\n",
    "    vec1 = np.array(vec1, dtype=float)\n",
    "    vec2 = np.array(vec2, dtype=float)\n",
    "\n",
    "\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "# Custom retrieval function using cosine similarity\n",
    "def find_similar_items(input_embedding, embeddings, threshold=0.5, top_k=3):\n",
    "    \"\"\"\n",
    "    Find the most similar items based on cosine similarity.\n",
    "\n",
    "    Args:\n",
    "    input_embedding (list): The query embedding.\n",
    "    embeddings (list): The knowledge store embeddings to search through for the best matches.\n",
    "    threshold (float): \n",
    "        Minimum similarity score for a match to be considered valid. \n",
    "        A higher threshold results in closer (better) matches, \n",
    "        while a lower threshold allows for more items to be returned but less relevant.\n",
    "    top_k (int): The number of top similar items to return.\n",
    "\n",
    "    Returns:\n",
    "    list: A sorted top-k most similar items.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate cosine similarity between the input embedding and all other embeddings\n",
    "    similarities = [(index, cosine_similarity_manual(input_embedding, vec)) for index, vec in enumerate(embeddings)]\n",
    "    \n",
    "    # Filter out any similarities below the threshold\n",
    "    filtered_similarities = [(index, sim) for index, sim in similarities if sim >= threshold]\n",
    "    \n",
    "    # Sort the filtered similarities by similarity score\n",
    "    sorted_indices = sorted(filtered_similarities, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "\n",
    "    # Return the top-k most similar items\n",
    "    return sorted_indices\n",
    "\n",
    "# Function to find matching items using RAG\n",
    "def find_matching_items_with_rag(df_items, item_descs):\n",
    "   \"\"\"Take the input item descriptions and find the most similar items based on cosine similarity for each description.\"\"\"\n",
    "   \n",
    "   # Select the embeddings from the DataFrame.\n",
    "   embeddings = df_items['embeddings'].tolist()\n",
    "\n",
    "   similar_items = []\n",
    "   for desc in item_descs:\n",
    "      # Generate the embedding for the input item\n",
    "      input_embedding = get_embeddings([desc])    \n",
    "      # Find the most similar items based on cosine similarity\n",
    "      similar_indices = find_similar_items(input_embedding, embeddings, threshold=0.6)\n",
    "      similar_items += [df_items.iloc[i] for i in similar_indices]\n",
    "    \n",
    "   return similar_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Module\n",
    "- gpt-4o-mini to analyze images and extract features like descriptions, styles, and types\n",
    "- Provide the URL of the image for analysis and request the model to identify relevant features\n",
    "\n",
    "Techniques used:\n",
    "- Structured Output as pre-defined JSON\n",
    "- Prompt Engineering: Clear and concise instructions\n",
    "- One Shot Examples: Providing relevant examples for the model to learn from\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze an Image using 4o-mini, and return a structred JSON output\n",
    "# with the following fields: \"items\", \"category\", and \"gender\".\n",
    "def analyze_image(image_base64, subcategories):\n",
    "    response = client.chat.completions.create(\n",
    "        model=GPT_MODEL,\n",
    "        messages=[\n",
    "            {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"\"\"Given an image of an item of clothing, analyze the item and generate a JSON output with the following fields: \"items\", \"category\", and \"gender\". \n",
    "                           Use your understanding of fashion trends, styles, and gender preferences to provide accurate and relevant suggestions for how to complete the outfit.\n",
    "                           The items field should be a list of items that would go well with the item in the picture. Each item should represent a title of an item of clothing that contains the style, color, and gender of the item.\n",
    "                           The category needs to be chosen between the types in this list: {subcategories}.\n",
    "                           You have to choose between the genders in this list: [Men, Women, Boys, Girls, Unisex]\n",
    "                           Do not include the description of the item in the picture. Do not include the ```json ``` tag in the output.\n",
    "                           \n",
    "                           Example Input: An image representing a black leather jacket.\n",
    "\n",
    "                           Example Output: {\"items\": [\"Fitted White Women's T-shirt\", \"White Canvas Sneakers\", \"Women's Black Skinny Jeans\"], \"category\": \"Jackets\", \"gender\": \"Women\"}\n",
    "                           \"\"\",\n",
    "                },\n",
    "                {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/jpeg;base64,{image_base64}\",\n",
    "                },\n",
    "                }\n",
    "            ],\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    # Extract relevant features from the response\n",
    "    features = response.choices[0].message.content\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Prompt with Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to encode the .jpg image into base64 string\n",
    "import base64\n",
    "\n",
    "def encode_image_to_base64(image_path):\n",
    "    with open(image_path, 'rb') as image_file:\n",
    "        encoded_image = base64.b64encode(image_file.read())\n",
    "        return encoded_image.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the images and select a test image\n",
    "image_path = \"data/sample_clothes/sample_images/\"\n",
    "test_images = [\"2133.jpg\", \"7143.jpg\", \"4226.jpg\"]\n",
    "\n",
    "# Encode the test image to base64\n",
    "reference_image = image_path + test_images[2]\n",
    "encoded_image = encode_image_to_base64(reference_image)\n",
    "# print(encoded_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [\"Casual Black Men's Jeans\", 'White Low-top Sneakers', \"Men's Lightweight Hoodie\"], 'category': 'T-shirts', 'gender': 'Men'}\n"
     ]
    }
   ],
   "source": [
    "# Select the unique subcategories from the DataFrame\n",
    "unique_subcategories = styles_df['articleType'].unique()\n",
    "# print(f\"There are {len(unique_subcategories)} unique subcategories, such as:\")\n",
    "# print(unique_subcategories)\n",
    "\n",
    "# Analyze the chosen sample image and return the results\n",
    "analysis = analyze_image(encoded_image, unique_subcategories)\n",
    "image_analysis = json.loads(analysis)\n",
    "\n",
    "# Display the image and the analysis results (structured JSON output of items, category, and gender)\n",
    "display(Image(filename=reference_image))\n",
    "print(image_analysis)\n",
    "# e.g. {'items': [\"Casual Black Men's Jeans\", 'White Low-top Sneakers', \"Men's Lightweight Hoodie\"], 'category': 'T-shirts', 'gender': 'Men'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizing RAG to recommend products based on the sampled image analysis\n",
    "\n",
    "- Output from image analysis includes Gender, Category, and a list of potential product descriptions that matches the analyzed image.\n",
    "- From the style database, filter for only items of the same gender (or unisex) and different category from the analyzed image.\n",
    "- We then perform a similarity search using the product descriptions, and return the top 5 most similar items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "603 Remaining Items\n",
      "[\"Casual Black Men's Jeans\", 'White Low-top Sneakers', \"Men's Lightweight Hoodie\"]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data/sample_clothes/sample_images/49373.jpg\" style=\"display:inline;margin:1px\"/><img src=\"data/sample_clothes/sample_images/26526.jpg\" style=\"display:inline;margin:1px\"/><img src=\"data/sample_clothes/sample_images/11821.jpg\" style=\"display:inline;margin:1px\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract the relevant features from the analysis\n",
    "item_descs = image_analysis['items']\n",
    "item_category = image_analysis['category']\n",
    "item_gender = image_analysis['gender']\n",
    "\n",
    "\n",
    "# Filter data such that we only look through the items of the same gender (or unisex) and different category\n",
    "filtered_items = styles_df.loc[styles_df['gender'].isin([item_gender, 'Unisex'])]\n",
    "filtered_items = filtered_items[filtered_items['articleType'] != item_category]\n",
    "print(str(len(filtered_items)) + \" Remaining Items\")\n",
    "\n",
    "# Find the most similar items based on the input item descriptions\n",
    "matching_items = find_matching_items_with_rag(filtered_items, item_descs)\n",
    "\n",
    "# Display the matching items (this will display top-k = 3 items for each description in the image analysis)\n",
    "html = \"\"\n",
    "paths = []\n",
    "for i, item in enumerate(matching_items):\n",
    "    item_id = item['id']\n",
    "        \n",
    "    # Path to the image file\n",
    "    image_path = f'data/sample_clothes/sample_images/{item_id}.jpg'\n",
    "    paths.append(image_path)\n",
    "    html += f'<img src=\"{image_path}\" style=\"display:inline;margin:1px\"/>'\n",
    "\n",
    "# Print the matching item description as a reminder of what we are looking for\n",
    "print(item_descs)\n",
    "# Display the image\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
