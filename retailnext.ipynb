{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Cookbook - GPT4o-mini with RAG\n",
    "\n",
    "Instructions before running this notebook:\n",
    "1. pip install -r requirements.txt to install the required packages\n",
    "2. Create a .env file with a valid OpenAI API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup dependencies and API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import ast\n",
    "import time\n",
    "import tiktoken\n",
    "import concurrent\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "from IPython.display import Image, display, HTML\n",
    "from typing import List\n",
    "\n",
    "# Load OpenAI API Key from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() # Load environment variables from .env file\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Open AI Models used in this demo\n",
    "GPT_MODEL = \"gpt-4o-mini\"\n",
    "EMBEDDING_MODEL = \"text-embedding-3-large\"\n",
    "EMBEDDING_COST_PER_1K_TOKENS = 0.00013\n",
    "GPT_IMAGE_MODEL = \"gpt-image-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a. Load the unstructured product catalog data into a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "styles_filepath = \"data/sample_clothes/sample_styles.csv\"\n",
    "styles_df = pd.read_csv(styles_filepath, on_bad_lines='skip')\n",
    "print(styles_df.head())\n",
    "print(\"Opened dataset successfully. Dataset has {} items of clothing.\".format(len(styles_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b. Helpful Utility Functions \n",
    "To create embeddings via batch + parallelized execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper functions for batching, and parallelizing the embedding process\n",
    "\n",
    "#  @retry is a decorator from the tenacity library that adds retry logic to make API calls more robust against temporary failures\n",
    "#  (e.g., failures due to API rate limits or network issues), it will:\n",
    "#       - Wait for a random exponential time between 1 and 40 seconds before retrying\n",
    "#       - Stop after 10 failed attempts\n",
    "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(10))\n",
    "def get_embeddings(input: List):\n",
    "    response = client.embeddings.create(\n",
    "        input=input,\n",
    "        model=EMBEDDING_MODEL\n",
    "    ).data\n",
    "    return [data.embedding for data in response]\n",
    "\n",
    "# Chunks large inputs into batches size n. \n",
    "def batchify(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx : min(ndx + n, l)]\n",
    "     \n",
    "# Batching (using batchify function) and parallel processing (using ThreadPoolExecutor)\n",
    "def embed_corpus( \n",
    "    corpus: List[str], \n",
    "    batch_size=64, \n",
    "    num_workers=8, \n",
    "    max_context_len=8191,\n",
    "    ):\n",
    "    # Encode the corpus, truncating to max_context_len\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\") #initializes tokenizer using cl100k_base encoding\n",
    "    encoded_corpus = [\n",
    "        encoded_article[:max_context_len] for encoded_article in encoding.encode_batch(corpus)\n",
    "    ]\n",
    "\n",
    "    # Calculate corpus statistics: the number of inputs, the total number of tokens, and the estimated cost to embed\n",
    "    num_tokens = sum(len(article) for article in encoded_corpus)\n",
    "    cost_to_embed_tokens = num_tokens / 1000 * EMBEDDING_COST_PER_1K_TOKENS\n",
    "    print(\n",
    "        f\"num_articles={len(encoded_corpus)}, num_tokens={num_tokens}, est_embedding_cost={cost_to_embed_tokens:.2f} USD\"\n",
    "    )\n",
    "\n",
    "    # Embed the corpus\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        \n",
    "        futures = [\n",
    "            executor.submit(get_embeddings, text_batch)\n",
    "            for text_batch in batchify(encoded_corpus, batch_size)\n",
    "        ]\n",
    "\n",
    "        # tqdm progress bar\n",
    "        with tqdm(total=len(encoded_corpus)) as pbar:\n",
    "            for _ in concurrent.futures.as_completed(futures):\n",
    "                pbar.update(batch_size)\n",
    "\n",
    "        embeddings = []\n",
    "        for future in futures:\n",
    "            data = future.result()\n",
    "            embeddings.extend(data)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "# Function to generate embeddings for a given column in a DataFrame\n",
    "def generate_embeddings(df, column_name):\n",
    "    descriptions = df[column_name].astype(str).tolist()\n",
    "    embeddings = embed_corpus(descriptions)\n",
    "\n",
    "    # Add the embeddings as a new column to the DataFrame\n",
    "    df['embeddings'] = embeddings\n",
    "    print(\"Embeddings created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Embeddings from Dataframe (containing unstructured text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates embeddings for each product description (productDisplayName) \n",
    "generate_embeddings(styles_df, 'productDisplayName')\n",
    "print(\"Writing embeddings to file ...\")\n",
    "styles_df.to_csv('data/sample_clothes/sample_styles_with_embeddings.csv', index=False)\n",
    "print(\"Embeddings successfully stored in sample_styles_with_embeddings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Uncomment this section and run if csv file already contains latest embeddings, and you want to skip embedding the corpus again from the previous step:\n",
    "# # Read the embeddings from the CSV file, load into a dataframe\n",
    "# styles_df = pd.read_csv('data/sample_clothes/sample_styles_with_embeddings.csv', on_bad_lines='skip')\n",
    "\n",
    "# # Convert the 'embeddings' column from string representations of lists to actual lists of floats\n",
    "# styles_df['embeddings'] = styles_df['embeddings'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "# Continue here if embeddings loaded into dataframe\n",
    "print(styles_df.head())\n",
    "print(\"Opened dataset successfully. Dataset has {} items of clothing along with their embeddings.\".format(len(styles_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building the Matching Algorithm\n",
    "\n",
    "- custom cosine similarity function to calculate semantic similarity between two product descriptions\n",
    "- `find_matching_items_with_rag` to search product catalogs and retrieve the most relevant items based on the input query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function for cosine similarity\n",
    "def cosine_similarity_manual(vec1, vec2):\n",
    "    \"\"\"Calculate the cosine similarity between two vectors.\"\"\"\n",
    "    vec1 = np.array(vec1, dtype=float)\n",
    "    vec2 = np.array(vec2, dtype=float)\n",
    "\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "# Custom retrieval function using cosine similarity\n",
    "def find_similar_items(input_embedding, embeddings, threshold=0.5, top_k=3):\n",
    "    \"\"\"\n",
    "    Find the most similar items based on cosine similarity.\n",
    "\n",
    "    Args:\n",
    "    input_embedding (list): The query embedding.\n",
    "    embeddings (list): The knowledge store embeddings to search through for the best matches.\n",
    "    threshold (float): Minimum similarity score for a match to be considered valid. \n",
    "    top_k (int): The number of top similar items to return.\n",
    "\n",
    "    Returns:\n",
    "    list: A sorted top-k most similar items.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate cosine similarity between the input embedding and all other embeddings\n",
    "    similarities = [(index, cosine_similarity_manual(input_embedding, vec)) for index, vec in enumerate(embeddings)]\n",
    "    \n",
    "    # Filter out any similarities below the threshold\n",
    "    filtered_similarities = [(index, sim) for index, sim in similarities if sim >= threshold]\n",
    "    \n",
    "    # Sort the filtered similarities by similarity score\n",
    "    sorted_indices = sorted(filtered_similarities, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "\n",
    "    # Return the top-k most similar items, sorted by similarity score\n",
    "    return sorted_indices\n",
    "\n",
    "# Function to find matching items using RAG\n",
    "def find_matching_items_with_rag(df_items, item_descs):\n",
    "   \"\"\"Take the input item descriptions and find the most similar items based on cosine similarity for each description.\"\"\"\n",
    "   \n",
    "   # Select the embeddings from the DataFrame.\n",
    "   embeddings = df_items['embeddings'].tolist()\n",
    "\n",
    "   similar_items = []\n",
    "   for desc in item_descs:\n",
    "      # Generate the embedding for the input item\n",
    "      input_embedding = get_embeddings([desc])    \n",
    "      # Find the most similar items based on cosine similarity\n",
    "      similar_indices = find_similar_items(input_embedding, embeddings, threshold=0.6)\n",
    "      similar_items += [df_items.iloc[i] for i in similar_indices]\n",
    "    \n",
    "   return similar_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Image Analysis Module\n",
    "- gpt-4o-mini to analyze images and extract features like gender, category, and generate new recommended product item descriptions\n",
    "- Provide the URL of the image for analysis and request the model to identify relevant features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze an Image, and return a structred JSON output with the following fields: \"items\", \"category\", and \"gender\".\n",
    "def analyze_image(image_base64, subcategories):\n",
    "    response = client.chat.completions.create(\n",
    "        model=GPT_MODEL, # gpt4o-mini\n",
    "        messages=[\n",
    "            {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"\"\"Given an image of an item of clothing, analyze the item and generate a JSON output with the following fields: \"items\", \"category\", and \"gender\". \\n\n",
    "                           Use your understanding of fashion trends, styles, and gender preferences to provide accurate and relevant suggestions for how to complete the outfit. \\n\n",
    "                           The category needs to be chosen between the types in this list: {subcategories}. \\n\n",
    "                           Use \"Tshirts\" if a Tshirt item is detected in the image taking more than half of the image. \\n\n",
    "                           The items field should be a list of items that would go well with the item in the picture but must describe items from a different category. Each item should represent a title of an item of clothing that contains the style, color, and gender of the item. \\n\n",
    "                           You have to choose between the genders in this list: [Men, Women, Boys, Girls, Unisex] \\n\n",
    "                           Do not include the description of the item in the picture. Do not include the ```json ``` tag in the output. \\n\n",
    "                           \n",
    "                           Example Input: An image representing a black leather jacket. \\n\n",
    "\n",
    "                           Example Output: {\"items\": [\"Fitted White Women's T-shirt\", \"White Canvas Sneakers\", \"Women's Black Skinny Jeans\"], \"category\": \"Jackets\", \"gender\": \"Women\"}\n",
    "                           \"\"\",\n",
    "                },\n",
    "                {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/jpeg;base64,{image_base64}\",\n",
    "                },\n",
    "                }\n",
    "            ],\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    # Extract relevant features from the response\n",
    "    features = response.choices[0].message.content\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing the Prompt with a Sample Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to encode the .jpg image into base64 string\n",
    "import base64\n",
    "\n",
    "def encode_image_to_base64(image_path):\n",
    "    with open(image_path, 'rb') as image_file:\n",
    "        encoded_image = base64.b64encode(image_file.read())\n",
    "        return encoded_image.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the images and select a test image\n",
    "image_path = \"data/sample_clothes/sample_images/\"\n",
    "# test_images = [\"2133.jpg\", \"7143.jpg\", \"4226.jpg\"]\n",
    "\n",
    "# Encode the test image to base64\n",
    "reference_image = \"data/test/portrait-1.jpg\"# reference_image = image_path + test_images[0]\n",
    "encoded_image = encode_image_to_base64(reference_image)\n",
    "# print(encoded_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the unique subcategories from the DataFrame\n",
    "unique_subcategories = styles_df['articleType'].unique()\n",
    "# print(f\"There are {len(unique_subcategories)} unique subcategories, such as:\")\n",
    "# print(unique_subcategories)\n",
    "\n",
    "# Analyze the chosen sample image and return the results (calls GPT4o-mini)\n",
    "analysis = analyze_image(encoded_image, unique_subcategories)\n",
    "image_analysis = json.loads(analysis)\n",
    "\n",
    "# Display the image selected and the analysis results (structured JSON output of items, category, and gender)\n",
    "display(Image(filename=reference_image))\n",
    "print(image_analysis)\n",
    "# e.g. {'items': [\"Casual Black Men's Jeans\", 'White Low-top Sneakers', \"Men's Lightweight Hoodie\"], 'category': 'T-shirts', 'gender': 'Men'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Utilizing RAG to recommend products based on the sampled image analysis\n",
    "\n",
    "- Output from image analysis includes Gender, Category, and a list of potential product descriptions that matches the analyzed image.\n",
    "- From the style database, filter for only items of the same gender (or unisex) and different category from the analyzed image.\n",
    "- We then perform a similarity search using the product descriptions, and return the top 5 most similar items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the relevant features from the analysis\n",
    "item_descs = image_analysis['items']\n",
    "item_category = image_analysis['category']\n",
    "item_gender = image_analysis['gender']\n",
    "\n",
    "# Filter data such that we only look through the items of the same gender (or unisex) and different category\n",
    "filtered_items = styles_df.loc[styles_df['gender'].isin([item_gender, 'Unisex'])]\n",
    "filtered_items = filtered_items[filtered_items['articleType'] != item_category]\n",
    "print(str(len(filtered_items)) + \" Remaining Items\")\n",
    "\n",
    "# Find the most similar items based on the input item descriptions\n",
    "matching_items = find_matching_items_with_rag(filtered_items, item_descs)\n",
    "\n",
    "# Display the matching items\n",
    "html = \"\"\n",
    "paths = []\n",
    "for i, item in enumerate(matching_items):\n",
    "    item_id = item['id']\n",
    "        \n",
    "    # Path to the image file\n",
    "    image_path = f'data/sample_clothes/sample_images/{item_id}.jpg'\n",
    "    paths.append(image_path)\n",
    "    html += f'<img src=\"{image_path}\" style=\"display:inline;margin:1px\"/>'\n",
    "\n",
    "# Print the matching item description as a reminder of what we are looking for\n",
    "print(item_descs)\n",
    "# Display the image\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Validation Guardrails\n",
    "\n",
    "After obtaining initial suggestions, we can send the original image and the suggested items back to the model. We can then ask GPT-4o mini to evaluate whether each suggested item would indeed be a good fit for the original outfit.\n",
    "\n",
    "This gives the model the ability to self-correct and adjust its own output based on feedback or additional information. By implementing these guardrails and enabling self-correction, we can enhance the reliability and usefulness of the model's output in the context of fashion analysis and recommendation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class GuardrailMatchResponse(BaseModel):\n",
    "    answer: str\n",
    "    reason: str\n",
    "\n",
    "def check_match(reference_image_base64, suggested_image_base64):\n",
    "    response = client.responses.parse(\n",
    "        model=GPT_MODEL,\n",
    "        input=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"input_text\",\n",
    "                    \"text\": (\n",
    "                        \"\"\"You will be given two images of two different items of clothing. \n",
    "                        Your goal is to decide if the items in the images would work in an outfit together. \n",
    "                        The first image is the reference item (the item that the user is trying to match with another item). \n",
    "                        You need to decide if the second item would work well with the reference item. \n",
    "                        The \"answer\" field must be either \"yes\" or \"no\", depending on whether you think the\n",
    "                        items would work well together. The \"reason\" field must be a short explanation of your reasoning \n",
    "                        for your decision. Do not include the descriptions of the 2 images.\"\"\"\n",
    "                    ),\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"input_image\",\n",
    "                    \"image_url\": f\"data:image/jpeg;base64,{reference_image_base64}\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"input_image\",\n",
    "                    \"image_url\": f\"data:image/jpeg;base64,{suggested_image_base64}\",\n",
    "                },\n",
    "            ]\n",
    "        }],\n",
    "        text_format=GuardrailMatchResponse,\n",
    "        max_output_tokens=300,\n",
    "    )\n",
    "\n",
    "    features = response.output_parsed\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Validate that recommended products truly complement the outfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the unique paths for the generated images\n",
    "paths = list(set(paths))\n",
    "\n",
    "for path in paths:\n",
    "    # Encode the test image to base64\n",
    "    suggested_image = encode_image_to_base64(path)\n",
    "    \n",
    "    # Check if the items match\n",
    "    match = check_match(encoded_image, suggested_image)\n",
    "    match_json = match.model_dump()\n",
    "    # print(match_json)\n",
    "\n",
    "    # Parse the JSON response\n",
    "    # match = json.loads(check_match(encoded_image, suggested_image))\n",
    "    \n",
    "    # Display the image and the analysis results\n",
    "    if match_json[\"answer\"] == 'yes':\n",
    "        display(Image(filename=path))\n",
    "        print(\"The items match!\")\n",
    "        print(match_json[\"reason\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Putting the new look together with Open AI Image Generation\n",
    "- Combine the original image with the new recommended items\n",
    "- Use OpenAI image generator to create this new look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select different styles for generation based on preference\n",
    "image_styles = [\n",
    "    \"High-Fashion Runway Illustration\",\n",
    "    \"Retro 80s/90s anime\",\n",
    "]\n",
    "image_style = image_styles[0]\n",
    "\n",
    "# Prompt for Image Generation\n",
    "prompt = f\"\"\"\n",
    "Keeping the original model outfit, add in new clothing accessories from the other images.\\n\n",
    "The resulting look would be a combined outfit showing off combined items as a new look on one model.\\n\n",
    "The new outfit must be a combination of the original outfit and the new items,\\n \n",
    "if there are more than one of the same items, just pick one, do not modify the item and keep its original design.\\n\n",
    "Return the image in {image_style} style, with a dark background.\\n\n",
    "Always include the full outfit look including shoes. The shoes must match and be the same as the reference images provided, if any.\n",
    "\"\"\"\n",
    "\n",
    "# Image Generation using the Open AI Images API and latest gpt-image-1 model\n",
    "img = client.images.edit(\n",
    "  image=[open(reference_image, \"rb\")] + [open(p, \"rb\") for p in paths],\n",
    "  prompt=prompt,\n",
    "  model=GPT_IMAGE_MODEL,\n",
    "  n=1,\n",
    "  size=\"1024x1536\",\n",
    "  quality=\"medium\",\n",
    "  background=\"auto\",\n",
    ")\n",
    "\n",
    "image_base64 = img.data[0].b64_json\n",
    "image_bytes = base64.b64decode(image_base64)\n",
    "\n",
    "# Save the image to a file\n",
    "timestamp = int(time.time()) # make it unique\n",
    "with open(f\"data/test/output_{timestamp}.png\", \"wb\") as f:\n",
    "    f.write(image_bytes)\n",
    "\n",
    "# Display the output image in the notebook\n",
    "from IPython.display import Image, display\n",
    "cache_buster = int(time.time())\n",
    "display(HTML(f'<img src=\"data/test/output_{timestamp}.png?{cache_buster}\" width=\"256\" height=\"384\"/>'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
